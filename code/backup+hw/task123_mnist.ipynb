{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f3684e-20f3-4e6b-b956-8a37c925b2cd",
   "metadata": {},
   "source": [
    "## 1.2.3 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866d0296-a4d6-45f5-aed6-17a024fb2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerenmizrahi/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/kerenmizrahi/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import plot_tsne\n",
    "\n",
    "import Trainers\n",
    "import classifiers as clf\n",
    "import mnist_123 as autoencoders\n",
    "#from mnist_123 import vae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bd9d8-1894-423f-aec4-5082f3b3b767",
   "metadata": {},
   "source": [
    "#### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f794960b-af32-49ba-970f-1036807b73ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPFElEQVR4nO2da3MT5f+Hr93sJptjd3NsG1pboJWqFQREx3GGB/oSfJe+AkcdH8DojIgyIhaLlJLSkCbN5nza4/+Bs/e0/ORv1R4WzGemUyDnXHvf3/ON5Pu+z1RnKvms38BUUwih0BRCCDSFEAJNIYRAUwgh0BRCCDSFEAJNIYRAylHvKEnSSb6P11JHTUZMV0IINIUQAk0hhEBTCCHQFEIINIUQAk0hhEBTCCHQFEIIdOSI+awlyzLRaBRZltE0jWQySSQSIZPJkEql8DyP8XiMbdu4rovv+7iui2VZDIdDXNdlPB4zHo+PHMmell4ZCNFolFwuh6ZplMtlVlZWSKfTvPXWW6ytreG6Ljs7OzSbTRzHYTQaYds2tVqNra0t+v0+1WqVarWK53l4nnfWH0nolYEQiUSIx+Mkk0my2SyLi4vous4777zD+++/j+M4ZLNZarUatm3T7/exLAtVVel2u6iqSrvdJhKJ4Ps+kiSFZkWEGoIkScTjcVRVZXZ2lg8//JBSqUSpVGJ5eZlkMkmhUECSJCKRCLquI0kSjuMwmUywbZtMJoOu64zHY549e8bu7i7j8ZharUa73RarJnjMcDg89VUSaggH9/y1tTU+++wzLl26JGyCoijCTsiyTKlUIp/PA+C6rvgd2Ilms4lpmrTbbe7cucOjR48YjUbUajWGwyGtVovJZDKFcFCSJCHLMoqiEIvF0HWdQqGAqqpomoYkSXieh+M4AGKbkSQJVVXFn4OtR1VVsaVVKhXa7Ta9Xo/JZEI0GsV1Xfr9PpPJRLwH3/exLEtAPQmFGoLneQyHQ3zfxzRNGo0GtVqNTCaDqqrIsky9Xqder2PbNp1Oh8FgQCqVYmFhgVQqRTweJ51OC68KIBaLce3aNRYXF7Esi06ng2VZ9Ho9ms2mWDme59HpdPjuu+/Y3Nw8sc/5SkCYTCa0Wi3q9Tq5XA7f9zEMA4B6vc7GxgaDwYBnz57RbDYpFovcuHGDUqmEYRgkEglkWSYej6NpGr7vUygUhJcUfOHj8Zh+vy+2MNu22d3dpV6v/3ch+L4v9mfHcbAsC8uysG0bx3HwfZ9er8f+/j79fp/9/X2azSaSJFGtVrEsi8FggOu6qKqKoihEIhFkWSYWixGNRolEIkQiEeCPWESSpEMQUqkUmqahqqqAdtxeVaghAOIqDbaLdruNpmkMBgMAHj58yJdffslgMKDZbNLv99E0jfv376NpGrquUywWhU3RdZ1kMsmbb77J/Pw8qqqSSqWIRqOoqipcWMdxcByHwWBANpsln88zmUzo9XrYtn2snzH0EHzfF8ZxPB4zHA5F5Ot5Hs+ePePnn3+m3+/T7/cZj8cAwiBnMhny+TyxWIy5uTnm5uYwDIN4PE4ikRA/gRMQjUaBP+C7rksymSSdTpNOp1EUheFw+N+CIEkSsVgMVVXRdZ1cLkc+nxcrwbZtBoOB2KIOupYBPNu2RRwQBGuj0YiHDx8yGo1IJBIUi0Xi8TiyLBOJRETcoaoq9XpdGO5gCzxuhRqCoijk83l0XefChQusr6+ztrZGq9WiVqvR7XbZ29uj2+0yHo//1I0cj8fs7+8jyzKtVovt7W0UReHevXtiFczNzQnjHdiNQqFAsVik1+uxvb0toJ+EqxpqCIFbmUqlmJmZEXvzcDhkMBjQ6XTE9hDECi8q2FYARqPR/9yeSCRoNBrE43EURREQFhYWxPbX7XYPOQPHrdBDSKVS5HI5kskkvu8zmUxoNBpsbGywv7/P3t7ev4pwHceh1+thWZawC5FI5FDg1mw2RcD2n4MQiUQwDIP5+Xmy2ayIGyqVCrdv32Zvb49qtfqvtgjbtjFNUzS3Bb+fP38uPKXA5sDRG7r+jkINIUg/aJpGNBo95CV1u106nc6/rg8E7uiLsizr37z1v6VQV9aClTA3Nycyoe12m263K7KkJ5nTOS2FeiVEIhFmZmaYnZ0lkUgwmUxot9svdUtfVYUSwsFMaCKRIJPJEIvFxE86naZUKhGNRslkMvT7fVEXmEwmIkYISpwn5dUcl0IJIRqNijTD+fPneffdd1EUBU3TRCJucXGRyWTCaDRiPB7T6/X46aefePLkCY7jMB6PcRyHTqcjqm1hVeggHDTGqVSK2dlZ3njjDVG4Acjn81y6dEm4rI7j0Gw2kWVZRMn9fl988Y1GYwrh7yoWi5HJZEin00SjUbE9AaKQExhl27axLAvP8ygWi6yurh4qWRqGgaIojEYjUVkLtqqwKHQQZFnGMAyWl5eZn58nnU6LfE4gy7IO2YEgEr569SrXr18XtsB1XarVKpubm7TbbW7dusWtW7cEtLCACB2EIGmXSqVIJBKoqnro9uALDlzUwCYEzQDZbPbQ/TOZDLIsY5omv/76K4qiCEM9hfASeZ5Ht9vl+fPnOI7DxsYGiURCBFWe52GaJtVqlclkwng8FjXihYUF8vk8qVSK+fl58bhisUgymWRtbY3d3V263S67u7vs7+8DJxMF/x2FEkKz2WQ4HFKv1zEMg2azieu6opNuZ2eHjY0NhsOhiBc0TWN5eZlCocDCwgI3b96kXC6TTqdZWlrC9336/T6KotBoNPjmm29otVqh2JZCBwEQnoyiKLRaLRqNBo7jMBwOcRyHWq3G8+fPxd9t2yYWi6FpGq7rEo1GMU2TVCol6hGyLDMzM0OhUMB1XRKJBIqiiKTcWYIIJYQgFTEYDNjc3KTZbB7yiLrd7qH8fnBbo9FgNBrR7XbxPA/DMLhy5QqpVIpMJkOhUODy5cvMzs6ytbWFaZqMRiPq9fqfprlPS6GEEBTUHcfh999/Z2tr69CVGmwhB//NsiwajQb7+/tUq1V2dnaIxWJMJhOuXbtGLBYjn89TLpep1+s8ePCAWq1Gq9Wi0+lMIbxMB13No94/MOBBnNBqtUQTcDabFSnxRCJBPp/H8zwU5Wy/Bumox6+9SsPkkiShKAqyLLO0tMT6+jq6rvPee+9x5coVPM+j0WjQ6XTY3Nzk888/5/Hjx8DxekpHfa5Qr4R/qiB1AbC9vU29XieRSOC6rmgGy+VyLC4u4nkeyWTyTLu0jx2Coigkk0nhkQT5nn6/z2AwOPUPGhhty7Jot9uijVLXdVKpFMlkUnTmBZ7Wab/HY4eQTCZZWVlB13WRiPN9n83NTTY3N19akD8pBQGe4zg8efKEaDQqWuvL5TJ7e3vkcjkMwxBF/dMuFB07BFVVMQxDTNXE43F836darYo2w9O80gLjDtDtdqnX6yiKgu/7JJNJEokEmqYRi8WwbftMbN+xQzjoChqGQalUwvd9xuMxpmkyHo9ptVqijXGqE4AQj8dZWlri4sWLLCwssLa2JrKgvV4P0zTZ3NycQjigY4cQ9HNGo1GSySS6rqMoCoZhYBgGnueJEaiT6nJ+mYLXC17zxdd+bbyjwAtpNpvk83nRlr68vMzNmzdpt9skEgl0Xaff77Ozs3Mqq8L3fVHY0TRNTOkETQOBAT8LEMcOIajrNptNer0ejuMgSRJLS0sUCgVarRau6xKPx6nVapimeaoQTNNE0zS63S79fl9kYh3HObP2mROB0Ov1aLVaYozVdV0xAmtZFtlsVhjsYrGIJElYlsVoNPqfLeM4r8yg21qW5UODIMEswmuzErrdLnfv3iWVSqEoCtevXycSiRCLxUSl7MaNG6ysrGCaJqurq5imydOnT/nll1/E1RnUDo5raE+SJBKJBNlslpmZGVE8Cjr5glX7WkAYDodsb28jSRIrKyt0Oh0ymYxoYwnGX33fp91uk81m6XQ63Lt3j2azSavVotVqiavz7yTw/krRaFSUTYO+1mDo5LXNogaG13EcyuXyoRmxoLUlqAGXy2XefvttOp0OjUaDer3OZDIR6eYgO/pPr9SgqHPu3DkxqRPUIl7byprv+1QqFb744guy2SwfffSRyNEkk0k0TSORSLCwsIDruszPz3P16lUmkwlbW1s8efIE0zT59ttvefjwoeiw+Kf9Q5FIhAsXLvDJJ58Qj8fRdV2URl9bCPDHSqhUKnS7XVZXVxmPx4fmiRVFIZVKATAzM0O5XMZxHJH8y2QybGxsiPsHycB/ooMrIRaLiXxSsBrOUicKYTgcsre3R6/X48cff8TzPNLpNBcvXmRubk60Oh5s8JJlGV3XWVhYYGZmho8//phyuYxpmjx69Ih2uy1KmH+VDJQkiWQyiWEYpFIplpaWKJVKuK7LxsYGlUqFSqVy5tH7iULodrsMh0MikQh7e3vcuXMHwzD49NNPWV9fxzAMVldX0XVdTMjIskyxWCSbzeI4DisrK2Iw5KuvvmJnZ0ccnxNMar5MkiRx7tw51tbW0HWdy5cvc/78eZE6+frrr4U7fZY6UQiBZyNJEu12W+T1G42GGPoeDodomnbosBBVVVFVVZxHETyuUCgwGo2wLItms0ksFvvL92AYhhg+DIYDg/YX0zRFx8ZZ6lQqa0HjbpCv+f7773n69CnFYpFKpUIul2Nubo7z58+L1HI8Hj9UpiwWi3zwwQdcunRJTPEf5cvTdZ1SqSTszN27d2k0GlQqlUOjsWepUytvBp7IYDCg1WohyzL5fJ7d3V3y+Tzr6+ti/9Z1XZziEhThC4UChmGIKPqoxjSwM7Zt88MPPwgIOzs7dDqdY41D/qlOtcYcuIKBmzkajWi32+K0lt3dXZFQC1IMQWwRxBXB8wQQgkLRy2TbtmiVDGKQZrMp5p7P2j2FM+62iMViGIYhzqCYm5sjmUyyvr4uVsbCwgK5XE6cQaEoCpPJRNSrg7jjRQUfa3d3l99++41Op8Pt27e5ffs2w+FQnPx1kt13r0S3RRARwx+Q79+/TywWo9frCVf1YLojGPgOWiKDnqHgsKkXFaRGHj16xP7+Pg8ePODBgwfT1viX6eCMWavV4unTp5imCfxxplEwOKKqKqPRiF6vJ+rEQcvKnz3f9vY2jx8/pt1uizT6WfeevqjQNX/Jskw6nSaTyRCJREgkEsIuKIpy6Mi1YCX8fx10QQdFkGLvdrunBuCorxM6CK+Tpv+dyyukKYQQaAohBJpCCIGmEEKgKYQQaAohBJpCCIGmEEKgI+eOwpRred00XQkh0BRCCDSFEAJNIYRAUwgh0BRCCDSFEAJNIYRAUwgh0P8Bg0byRneXMsgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def freeze_seeds(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "freeze_seeds(42)\n",
    "\n",
    "# normalize data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "dataset_path = '/datasets/cv_datasets/data'\n",
    "batch_size = 64\n",
    "\n",
    "# Load MNIST from the given path\n",
    "ds_train = datasets.MNIST(root=dataset_path, train=True, download=False, transform=transform)\n",
    "ds_validation = datasets.MNIST(root=dataset_path, train=False, download=False, transform=transform)\n",
    "\n",
    "# dataset split: \n",
    "# train + test (during training)\n",
    "# validation\n",
    "\n",
    "# Split  80% train  20% test\n",
    "train_size = int(0.8 * len(ds_train))  \n",
    "test_size = len(ds_train) - train_size  \n",
    "ds_train, ds_test = random_split(ds_train, [train_size, test_size])\n",
    "\n",
    "# DataLoaders\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(ds_validation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Example - [batch_size, 1, 28, 28]\n",
    "for image, label in dl_train:\n",
    "    print(image.shape)\n",
    "    print(label[0])\n",
    "    img = image[0].squeeze(0)  \n",
    "    plt.figure(figsize=(1, 1))  \n",
    "    plt.imshow(img.cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')  \n",
    "    plt.show()\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163932e6-a22f-4adb-98cd-471dfa7afa5f",
   "metadata": {},
   "source": [
    "#### Model - 1.2.1 changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b027c223-99b8-4091-8aad-bddf9b7e64e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainers\u001b[38;5;241m.\u001b[39mautoencoderTrainer(encoder, decoder, dl_train, dl_test, loss_fn, optimizer, num_epochs, device)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainAutoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpimg\u001b[39;00m\n\u001b[1;32m     15\u001b[0m plot_tsne(encoder, dl_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n",
      "File \u001b[0;32m~/mini_project_w25/code/Trainers.py:47\u001b[0m, in \u001b[0;36mautoencoderTrainer.trainAutoencoder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     45\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl_train:\n\u001b[1;32m     48\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#self.optimizer.zero_grad()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py:920\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    917\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    919\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 920\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (std \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = autoencoders.Encoder_mnist().to(device)\n",
    "decoder = autoencoders.Decoder_mnist().to(device)\n",
    "\n",
    "loss_fn = nn.L1Loss() #  mean absolute error (reconstruction error) \n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),  lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "trainer = Trainers.autoencoderTrainer(encoder, decoder, dl_train, dl_test, loss_fn, optimizer, num_epochs, device)\n",
    "trainer.trainAutoencoder() \n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "plot_tsne(encoder, dl_test, \"MNIST\", device)\n",
    "\n",
    "orig_img = 'image_tsne_mnist.png'\n",
    "latent_img = 'latent_tsne_mnist.png'\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "img = mpimg.imread(orig_img)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  \n",
    "\n",
    "img = mpimg.imread(latent_img)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c819028d-9809-481c-a6a9-330b78b2c750",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x256 and 64x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m freeze_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mclfTrainer(classifier, encoder, dl_train, dl_test,\n\u001b[1;32m     14\u001b[0m                                  hyperparams, freeze_encoder, device)\n\u001b[0;32m---> 15\u001b[0m train_acc, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mini_project_w25/code/classifiers.py:81\u001b[0m, in \u001b[0;36mclfTrainer.trainClassifier\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m     encoded_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(img)\n\u001b[1;32m     80\u001b[0m encoded_img \u001b[38;5;241m=\u001b[39m encoded_img\u001b[38;5;241m.\u001b[39mview(encoded_img\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(output, label)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mini_project_w25/code/classifiers.py:29\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x256 and 64x64)"
     ]
    }
   ],
   "source": [
    "classifier = clf.Classifier(input_size=).to(device) \n",
    "\n",
    "hyperparams = {\n",
    "    'loss_fn': torch.nn.CrossEntropyLoss(),\n",
    "    'optimizer' : optim.Adam,\n",
    "    'weight_decay' : 0.0,\n",
    "    'learning_rate': 0.001,  \n",
    "    'num_epochs': 5,            \n",
    "}\n",
    "\n",
    "freeze_encoder = True\n",
    "\n",
    "trainer = clf.clfTrainer(classifier, encoder, dl_train, dl_test,\n",
    "                                 hyperparams, freeze_encoder, device)\n",
    "train_acc, test_acc = trainer.trainClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845078e-5909-44a7-beca-001afc8e27a7",
   "metadata": {},
   "source": [
    "#### Model - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3250ee16-5207-4c97-bb50-a10d74eee91a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m encoder \u001b[38;5;241m=\u001b[39m autoencoders\u001b[38;5;241m.\u001b[39mEncoderCNN(in_channels\u001b[38;5;241m=\u001b[39mim_size[\u001b[38;5;241m0\u001b[39m], out_channels\u001b[38;5;241m=\u001b[39mh_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m decoder \u001b[38;5;241m=\u001b[39m autoencoders\u001b[38;5;241m.\u001b[39mDecoderCNN(in_channels\u001b[38;5;241m=\u001b[39mh_dim, out_channels\u001b[38;5;241m=\u001b[39mim_size[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m vae \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m vae_dp \u001b[38;5;241m=\u001b[39m DataParallel(vae)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Show model \u001b[39;00m\n",
      "File \u001b[0;32m~/mini_project_w25/code/mnist_123.py:145\u001b[0m, in \u001b[0;36mVAE.__init__\u001b[0;34m(self, features_encoder, features_decoder, in_size, z_dim)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_decoder \u001b[38;5;241m=\u001b[39m features_decoder\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_dim \u001b[38;5;241m=\u001b[39m z_dim\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Add more layers as needed for encode() and decode().\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_mu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features, z_dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/mini_project_w25/code/mnist_123.py:161\u001b[0m, in \u001b[0;36mVAE._check_features\u001b[0;34m(self, in_size)\u001b[0m\n\u001b[1;32m    159\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_encoder(x)\n\u001b[1;32m    160\u001b[0m xr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_decoder(h)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m xr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Return the shape and number of encoded features\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:], torch\u001b[38;5;241m.\u001b[39mnumel(h) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import DataParallel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "im_size = ds_train[0][0].shape\n",
    "h_dim = 128 #latent space dimention\n",
    "z_dim = 2\n",
    "\n",
    "'''\n",
    "# Hyperparams\n",
    "h_dim=512,\n",
    "z_dim=256,\n",
    "\n",
    "'''\n",
    "\n",
    "# Model\n",
    "encoder = autoencoders.EncoderCNN(in_channels=im_size[0], out_channels=h_dim).to(device)\n",
    "decoder = autoencoders.DecoderCNN(in_channels=h_dim, out_channels=im_size[0]).to(device)\n",
    "vae = autoencoders.VAE(encoder, decoder, im_size, z_dim).to(device)\n",
    "vae_dp = DataParallel(vae).to(device)\n",
    "\n",
    "# Show model \n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41639b-c5e3-40a0-bf2c-ef5457091be5",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b8ea9-5e22-4f57-a9b6-4874927b3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "more hyperparams\n",
    "x_sigma2=0.0004,\n",
    "learn_rate=0.0002,\n",
    "betas=(0.9, 0.999),\n",
    "'''\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learn_rate, betas=betas)\n",
    "\n",
    "def loss_fn(x, xr, z_mu, z_log_sigma2):\n",
    "    return autoencoder.vae_loss(x, xr, z_mu, z_log_sigma2, x_sigma2)\n",
    "\n",
    "trainer = VAETrainer(vae_dp, dl_train, dl_test, loss_fn, optimizer, num_epochs, device)\n",
    "checkpoint_file = 'checkpoints/vae'\n",
    "checkpoint_file_final = f'{checkpoint_file}_final'\n",
    "if os.path.isfile(f'{checkpoint_file}.pt'):\n",
    "    os.remove(f'{checkpoint_file}.pt')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
